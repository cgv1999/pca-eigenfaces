# PCA и Eigenfaces: линейная алгебра данных изображений

Данный репозиторий содержит исследовательский ноутбук, посвящённый методу
главных компонент (PCA) и его интерпретации через линейную алгебру и
сингулярное разложение (SVD).

Цель работы - понять PCA не как "алгоритм из библиотеки",  
а как **каноническую форму линейного оператора**, возникающего при работе
с центрированными данными.

Исследование вдохновлено книгой:

**Jean Gallier**  
*Linear Algebra for Computer Vision, Robotics, and Machine Learning*

---

## О чём этот ноутбук

В ноутбуке последовательно разбираются следующие идеи:

- представление изображений как векторов в высокоразмерном пространстве признаков;
- переход от аффинного подпространства к линейному посредством центрирования данных;
- интерпретация центрированной матрицы данных как линейного оператора
  
  $$
  X_c : \mathbb{R}^d \to \mathbb{R}^n
  $$
  
- геометрический смысл скалярных произведений и вариации данных;
- постановка задачи PCA как задачи максимизации дисперсии;
- сингулярное разложение (SVD) и его связь с PCA;
- eigenfaces как главные направления вариации изображений лиц;
- сжатие и восстановление изображений;
- поиск ближайших соседей в пространстве PCA;
- линейная генерация новых изображений из базиса признаков.

---

## Датасет

Используется датасет **Olivetti Faces**:

- 400 изображений лиц
- размер изображений: 64 × 64 (4096 признаков)
- оттенки серого

Каждое изображение рассматривается как вектор

$$
x^i \in \mathbb{R}^{4096}
$$

---

## Ключевая идея

После центрирования данных матрица

$$
X_c \in \mathbb{R}^{n \times d}
$$

рассматривается как линейный оператор, который каждому направлению
в пространстве признаков сопоставляет координаты всех изображений
вдоль этого направления.

Сингулярное разложение

$$
X_c = U \Sigma V^T
$$

даёт каноническую форму этого оператора:

- столбцы матрицы **V** — главные направления (eigenfaces);
- сингулярные числа **Σ** отражают величину вариации данных;
- матрица **U** кодирует распределение изображений вдоль найденных направлений.

Таким образом, PCA интерпретируется как геометрическое описание облака
данных в пространстве признаков.

---

## Реализация

- PCA реализован **с нуля** через `numpy.linalg.svd`
- функции PCA из `sklearn` не используются
- основной акцент сделан на интерпретации, а не на оптимизации

---

## Статус проекта

Исследовательский и образовательный проект.

Цель — глубокое понимание геометрии данных и линейных методов,
а не построение прикладного ML-пайплайна.

---
